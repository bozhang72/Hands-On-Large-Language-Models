{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bozhang72/Hands-On-Large-Language-Models/blob/main/%E2%AD%90_KeyDoc_MLE_Interview_coding_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcec2e86",
      "metadata": {
        "id": "fcec2e86"
      },
      "source": [
        "## 1. Core ML Knowledge\n",
        "- [ ] Linear & Logistic Regression: interpretation, regularization (L1/L2)\n",
        "- [ ] Decision Trees, Random Forests, XGBoost: pros/cons, overfitting control\n",
        "- [ ] K-Means, PCA, t-SNE: dimensionality reduction & clustering\n",
        "- [ ] Loss functions: MSE, cross-entropy, hinge, contrastive loss\n",
        "- [ ] Evaluation metrics: accuracy, AUC, precision/recall, log loss\n",
        "- [ ] Model selection: cross-validation, early stopping, learning curves\n",
        "- [ ] Overfitting & bias-variance tradeoff\n",
        "- [ ] Feature importance: SHAP, gain, permutation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear && Logistic Regression"
      ],
      "metadata": {
        "id": "djP8RWF6zqRb"
      },
      "id": "djP8RWF6zqRb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogisticRegressionL2(torch.nn.Module):\n",
        "    def __init__(self, input_dim, l2_lambda=0.01):\n",
        "        super().__init__()\n",
        "        self.weights = torch.nn.Parameter(torch.randn(input_dim, 1))  # shape: (D, 1)\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(1))\n",
        "        self.l2_lambda = l2_lambda\n",
        "\n",
        "    def forward(self, X):\n",
        "        logits = X @ self.weights + self.bias  # shape: (N, 1)\n",
        "        return torch.sigmoid(logits)  # probability output\n",
        "\n",
        "    def loss(self, X, y):\n",
        "        preds = self.forward(X).squeeze()\n",
        "        bce = F.binary_cross_entropy(preds, y)\n",
        "        l2_penalty = self.l2_lambda * torch.sum(self.weights ** 2)\n",
        "        return bce + l2_penalty\n",
        "\n",
        "\n",
        "# Example data (binary classification)\n",
        "torch.manual_seed(42)\n",
        "X = torch.randn(100, 5)  # 100 samples, 5 features\n",
        "true_w = torch.tensor([[1.], [-1.], [0.5], [0.], [2.]])\n",
        "y_prob = torch.sigmoid(X @ true_w).squeeze()\n",
        "y = (y_prob > 0.5).float()  # binary labels\n",
        "\n",
        "# Model + optimizer\n",
        "model = LogisticRegressionL2(input_dim=5, l2_lambda=0.1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    loss = model.loss(X, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TMgjTzbx9lF",
        "outputId": "41fbcef0-cbdb-409c-b893-8513a64f193c"
      },
      "id": "7TMgjTzbx9lF",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0018\n",
            "Epoch 100, Loss: 0.5242\n",
            "Epoch 200, Loss: 0.5239\n",
            "Epoch 300, Loss: 0.5239\n",
            "Epoch 400, Loss: 0.5239\n",
            "Epoch 500, Loss: 0.5239\n",
            "Epoch 600, Loss: 0.5239\n",
            "Epoch 700, Loss: 0.5239\n",
            "Epoch 800, Loss: 0.5239\n",
            "Epoch 900, Loss: 0.5239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree, Random Forest, XGBoost\n",
        "\n",
        "# üå≥ Decision Trees vs. Random Forests vs. XGBoost\n",
        "\n",
        "## 1. Decision Trees\n",
        "\n",
        "- **Concept**: Tree-based model that splits data based on feature thresholds to minimize impurity (e.g., Gini or entropy).\n",
        "- **Pros**:\n",
        "  - Easy to interpret\n",
        "  - Fast to train\n",
        "  - Handles non-linearities\n",
        "  - Requires minimal data prep\n",
        "- **Cons**:\n",
        "  - Overfits easily\n",
        "  - Unstable (small data change ‚Üí different tree)\n",
        "  - Low predictive power as a single model\n",
        "- **Use Case**: Quick baseline, interpretable decision rules\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Random Forests\n",
        "\n",
        "- **Concept**: Ensemble of many decision trees trained on random subsets of data + features (bagging). Final output is majority vote or average.\n",
        "- **Pros**:\n",
        "  - Reduces overfitting\n",
        "  - High accuracy\n",
        "  - Robust to noise\n",
        "  - Handles missing data & outliers well\n",
        "- **Cons**:\n",
        "  - Slower than a single tree\n",
        "  - Less interpretable\n",
        "  - Can be memory-intensive\n",
        "- **Use Case**: General-purpose, good first choice for tabular data\n",
        "\n",
        "---\n",
        "\n",
        "## 3. XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "- **Concept**: Gradient boosting framework that builds trees sequentially, each correcting the previous one's errors using gradient descent.\n",
        "- **Pros**:\n",
        "  - State-of-the-art performance on tabular data\n",
        "  - Regularization (L1/L2)\n",
        "  - Missing value handling\n",
        "  - Fast and scalable\n",
        "- **Cons**:\n",
        "  - More complex to tune (learning rate, depth, etc.)\n",
        "  - Slower training than Random Forests\n",
        "- **Use Case**: Competitions, production models needing best performance\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Summary Comparison Table\n",
        "\n",
        "| Model            | Overfitting | Accuracy     | Interpretability | Speed     | Feature Importance |\n",
        "|------------------|-------------|--------------|------------------|-----------|--------------------|\n",
        "| Decision Tree    | High        | Low‚ÄìMedium   | High             | Very Fast | ‚úÖ Yes             |\n",
        "| Random Forest    | Low         | High         | Medium           | Moderate  | ‚úÖ Yes (avg)       |\n",
        "| XGBoost          | Very Low    | Very High    | Low              | Slower    | ‚úÖ Yes (boosted gain) |\n"
      ],
      "metadata": {
        "id": "crOqLslfzuXK"
      },
      "id": "crOqLslfzuXK"
    },
    {
      "cell_type": "markdown",
      "id": "81754722",
      "metadata": {
        "id": "81754722"
      },
      "source": [
        "## 2. Deep Learning Engineering\n",
        "- [ ] Neural nets: architecture, forward/backward pass\n",
        "- [ ] CNNs: convolution, pooling, receptive field\n",
        "- [ ] RNNs/LSTMs/GRUs: sequence modeling, time steps\n",
        "- [ ] Transformers: attention, multi-head, positional encoding\n",
        "- [ ] Model debugging: vanishing gradients, dead neurons, exploding gradients\n",
        "- [ ] Training tricks: batch norm, dropout, LR schedules, warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56df1a8e",
      "metadata": {
        "id": "56df1a8e"
      },
      "source": [
        "## 3. PyTorch Coding Proficiency\n",
        "- [ ] `nn.Module`, forward(), register_buffer\n",
        "- [ ] Custom loss functions, metrics\n",
        "- [ ] Data pipeline with `Dataset` & `DataLoader`\n",
        "- [ ] Training loop: batching, optimizer steps, validation\n",
        "- [ ] Save/load models with `state_dict`\n",
        "- [ ] TorchScript, quantization, `torch.compile` (for deployment)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ef4ec5",
      "metadata": {
        "id": "75ef4ec5"
      },
      "source": [
        "## 4. ML System Design\n",
        "- [ ] Offline vs Online inference: batch scoring vs real-time predictions\n",
        "- [ ] Model training pipeline: from raw data ‚Üí features ‚Üí training\n",
        "- [ ] Feature store: freshness, reuse, consistency with production\n",
        "- [ ] Model serving: REST/gRPC endpoint, latency/batching tradeoffs\n",
        "- [ ] Retraining strategies: time-based, performance-based, online learning\n",
        "- [ ] Monitoring: latency, throughput, drift, A/B testing metrics\n",
        "- [ ] Model versioning, rollback, shadow deployment\n",
        "- [ ] GPU vs CPU serving trade-offs (e.g. large embedding tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb5df94",
      "metadata": {
        "id": "ddb5df94"
      },
      "source": [
        "## 5. Real-World ML Problem-Solving\n",
        "- [ ] Build an end-to-end ML solution: ingestion, features, model, metrics\n",
        "- [ ] Tradeoff discussion: model complexity vs latency, accuracy vs robustness\n",
        "- [ ] Production bugs: feature drift, label leakage, data inconsistency\n",
        "- [ ] Model impact analysis: lift in CTR, retention, engagement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "686f9335",
      "metadata": {
        "id": "686f9335"
      },
      "source": [
        "## 6. Coding Interviews (Data Structures + ML Coding)\n",
        "- [ ] Arrays, strings, hashmaps, sliding window, two pointers\n",
        "- [ ] Matrix operations: multiplication, diagonal, transpose\n",
        "- [ ] Graph algorithms (BFS/DFS)\n",
        "- [ ] Binary search, heap, queue\n",
        "- [ ] Implement basic models: logistic regression, dot-product similarity\n",
        "- [ ] Vectorized PyTorch/Numpy code for mini ML problems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdd2b2b8",
      "metadata": {
        "id": "cdd2b2b8"
      },
      "source": [
        "## 7. Recommender/NLP (if relevant)\n",
        "- [ ] Pointwise vs Pairwise vs Listwise ranking\n",
        "- [ ] Implicit feedback, negative sampling\n",
        "- [ ] Collaborative filtering vs content-based filtering\n",
        "- [ ] Embedding generation, cosine similarity, ANN search\n",
        "- [ ] Tokenization, embeddings (word2vec/BERT), transformer fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2dd3c00",
      "metadata": {
        "id": "f2dd3c00"
      },
      "source": [
        "## 8. Behavioral & Communication\n",
        "- [ ] Use STAR framework to talk through ML projects\n",
        "- [ ] Metrics-focused results (e.g., latency reduced 40%, AUC +0.03)\n",
        "- [ ] Talk about tradeoffs: e.g., 'We chose X over Y because latency mattered more than accuracy'\n",
        "- [ ] How you handled bad data, retraining failures, rollout problems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee7ea59",
      "metadata": {
        "id": "bee7ea59"
      },
      "source": [
        "## Bonus for Staff-Level Candidates\n",
        "- [ ] Leading project roadmap: how you align with PM, DS, infra\n",
        "- [ ] Design doc writing: include assumptions, tradeoffs, metrics\n",
        "- [ ] Mentoring or setting coding/design standards\n",
        "- [ ] Influencing architecture across teams (e.g. shared embedding service)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}